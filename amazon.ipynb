{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2157,"sourceType":"datasetVersion","datasetId":18}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-25T06:57:13.789141Z","iopub.execute_input":"2023-08-25T06:57:13.789640Z","iopub.status.idle":"2023-08-25T06:57:13.818283Z","shell.execute_reply.started":"2023-08-25T06:57:13.789586Z","shell.execute_reply":"2023-08-25T06:57:13.817207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from accelerate import Accelerator\n# accelerator = Accelerator()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T18:42:07.935047Z","iopub.execute_input":"2023-08-24T18:42:07.935424Z","iopub.status.idle":"2023-08-24T18:42:21.484964Z","shell.execute_reply.started":"2023-08-24T18:42:07.935389Z","shell.execute_reply":"2023-08-24T18:42:21.484008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/amazon-fine-food-reviews/Reviews.csv\")\ntrain=train.dropna().reset_index()\ntest=pd.read_csv(\"/kaggle/input/amazon-fine-food-reviews/Reviews.csv\")\nsub=pd.read_csv(\"/kaggle/input/amazon-fine-food-reviews/Reviews.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-21T04:34:34.746039Z","iopub.execute_input":"2023-08-21T04:34:34.746575Z","iopub.status.idle":"2023-08-21T04:34:50.796154Z","shell.execute_reply.started":"2023-08-21T04:34:34.746532Z","shell.execute_reply":"2023-08-21T04:34:50.795059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv(\"/kaggle/input/amazon-fine-food-reviews/Reviews.csv\")\n# .iloc[:5000]\ndf","metadata":{"execution":{"iopub.status.busy":"2023-09-01T20:20:40.645805Z","iopub.execute_input":"2023-09-01T20:20:40.646206Z","iopub.status.idle":"2023-09-01T20:20:48.190633Z","shell.execute_reply.started":"2023-09-01T20:20:40.646160Z","shell.execute_reply":"2023-09-01T20:20:48.188522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-09-01T20:20:53.140895Z","iopub.execute_input":"2023-09-01T20:20:53.141371Z","iopub.status.idle":"2023-09-01T20:20:54.167367Z","shell.execute_reply.started":"2023-09-01T20:20:53.141332Z","shell.execute_reply":"2023-09-01T20:20:54.166243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n\n# Adding a new column based on existing column values using custom function\ndef custom_transformation(age):\n    if age >= 4:\n        return 2\n    elif age==3:\n        return 1\n    else:\n        return 0\n\ndf['binary'] = df['Score'].apply(custom_transformation)\ndf\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T10:00:35.298277Z","iopub.execute_input":"2023-08-25T10:00:35.298714Z","iopub.status.idle":"2023-08-25T10:00:35.387467Z","shell.execute_reply.started":"2023-08-25T10:00:35.298676Z","shell.execute_reply":"2023-08-25T10:00:35.386498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n\n# Adding a new column based on existing column values using custom function\ndef custom_transformation(age):\n    if age >= 3:\n        return 1\n    else:\n        return 0\n\ndf['binary'] = df['Score'].apply(custom_transformation)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-09-01T20:21:05.340873Z","iopub.execute_input":"2023-09-01T20:21:05.341970Z","iopub.status.idle":"2023-09-01T20:21:05.798420Z","shell.execute_reply.started":"2023-09-01T20:21:05.341931Z","shell.execute_reply":"2023-09-01T20:21:05.797351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_counts = df['binary'].value_counts()\nprint(\"Number of rows for each label:\")\nprint(label_counts)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T20:21:15.717180Z","iopub.execute_input":"2023-09-01T20:21:15.717609Z","iopub.status.idle":"2023-09-01T20:21:15.736476Z","shell.execute_reply.started":"2023-09-01T20:21:15.717575Z","shell.execute_reply":"2023-09-01T20:21:15.735409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Define the number of samples for each class\nnum_samples_class_1 = 486404\nnum_samples_class_0 = 82007\n\n# Calculate the number of true positives, false positives, true negatives, and false negatives\ntrue_positives = num_samples_class_1 * 0.9583\nfalse_positives = num_samples_class_0 * 0.2173\ntrue_negatives = num_samples_class_0 * 0.7827\nfalse_negatives = num_samples_class_1 * 0.0417\n\n# Create the confusion matrix\nconfusion_matrix_values = np.array([[true_negatives, false_positives],\n                                    [false_negatives, true_positives]])\n\n# # Plot the confusion matrix using seaborn\n# plt.figure(figsize=(8, 6))\nsns.heatmap(confusion_matrix_values, annot=True, fmt='.0f', cmap='Blues',\n            xticklabels=['Predicted 0', 'Predicted 1'],\n            yticklabels=['Actual 0', 'Actual 1'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T20:47:32.265471Z","iopub.execute_input":"2023-09-01T20:47:32.265844Z","iopub.status.idle":"2023-09-01T20:47:32.593685Z","shell.execute_reply.started":"2023-09-01T20:47:32.265814Z","shell.execute_reply":"2023-09-01T20:47:32.592727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = 'ternary.csv'\ndf.to_csv(output_file, index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:04:37.570635Z","iopub.execute_input":"2023-08-25T08:04:37.571123Z","iopub.status.idle":"2023-08-25T08:04:37.614679Z","shell.execute_reply.started":"2023-08-25T08:04:37.571076Z","shell.execute_reply":"2023-08-25T08:04:37.612233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"y=df[\"binary\"].values\nimport seaborn as sns\nsns.displot(y)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:53:54.858277Z","iopub.execute_input":"2023-08-31T12:53:54.858686Z","iopub.status.idle":"2023-08-31T12:53:57.363195Z","shell.execute_reply.started":"2023-08-31T12:53:54.858651Z","shell.execute_reply":"2023-08-31T12:53:57.362227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\n# Extract the binary column values\ny = df[\"binary\"].values\n\n# Create a labeled frequency plot using Seaborn\nsns.set(style=\"whitegrid\")  # Set the style of the plot\nplt.figure(figsize=(6, 4))  # Set the figure size\nplot = sns.countplot(x=y)  # Create the count plot\n\n# Set labels and title\nplot.set(xticklabels=[\"Label 0\", \"Label 1\"], xlabel=\"Label\", ylabel=\"Frequency\")\nplt.title(\"Label Frequency Plot\")\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-26T07:57:34.190820Z","iopub.execute_input":"2023-08-26T07:57:34.191209Z","iopub.status.idle":"2023-08-26T07:57:35.589032Z","shell.execute_reply.started":"2023-08-26T07:57:34.191177Z","shell.execute_reply":"2023-08-26T07:57:35.588015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\nimport emoji\ndef dataPreprocessing(x):    \n    x = x.lower()\n    x = removeHTML(x)\n    x = emoji.demojize(x, delimiters=(\" \", \" \"))\n    x = re.sub(\"@\\w+\", '',x) # removing mentions (@)\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(r\"[^\\w\\s]\", '',x) # to remove symbols\n    x = re.sub(\"http\\w+\", '',x)\n    x = re.sub(\"\\s[a-z]\\s\", '',x)\n    x = x.strip()\n    return x\n# df[\"Summary\"]=df[\"Summary\"].apply(lambda x: dataPreprocessing(x))\ndf[\"Text\"]=df[\"Text\"].apply(lambda x: dataPreprocessing(x))\n\ndf","metadata":{"execution":{"iopub.status.busy":"2023-08-26T07:21:12.998790Z","iopub.execute_input":"2023-08-26T07:21:12.999144Z","iopub.status.idle":"2023-08-26T07:21:13.365057Z","shell.execute_reply.started":"2023-08-26T07:21:12.999114Z","shell.execute_reply":"2023-08-26T07:21:13.364052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, f1_score\nfrom scipy.sparse import hstack\nstopwords = stopwords.words('english')\nstopwords = stopwords[:116]\nstopwords = stopwords.extend(['d', 'll', 're', 's', 've'])\n# data=pd.concat([train_text1,test_text1],axis=0).reset_index()[\"Summary\"]\n# data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n\ntokenizer = RegexpTokenizer(r'\\w+')\nstemmer = PorterStemmer()\n\n# Load stopwords and extend the list\nstop_words = stopwords.words('english')\nadditional_stop_words = ['d', 'll', 're', 's', 've']\nstop_words = stop_words[:116] + additional_stop_words\n\n# Preprocess the text data\ndef preprocess_text(text):\n    tokens = tokenizer.tokenize(text.lower())  # Tokenize and convert to lowercase\n    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]  # Apply stemming and remove stopwords\n    preprocessed_text = ' '.join(tokens)  # Join tokens back into a string\n    return preprocessed_text\n\n# Apply preprocessing to each row of the 'summary' column\n# df[\"Summary\"]=df[\"Summary\"].apply(lambda x: dataPreprocessing(x))\ndf[\"Text\"]=df[\"Text\"].apply(preprocess_text)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-08-26T07:21:18.372143Z","iopub.execute_input":"2023-08-26T07:21:18.372571Z","iopub.status.idle":"2023-08-26T07:21:20.060089Z","shell.execute_reply.started":"2023-08-26T07:21:18.372535Z","shell.execute_reply":"2023-08-26T07:21:20.059135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Load data from CSV\n# data = pd.read_csv(\"your_data.csv\")  # Replace with your CSV file\n\n# Assuming the CSV has columns \"text\" and \"label\"\ntexts = df[\"Summary\"].tolist()\nlabels = df[\"binary\"].tolist()  # Make sure the label column is encoded as integers (0, 1, 2)\n\n# Split data into train and test sets\nxtrain, xtest, ytrain, ytest = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = \"roberta-large\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 sentiment classes\n\n# Tokenize the input data\nencoded_train_inputs = tokenizer(xtrain, padding=True, truncation=True, return_tensors=\"pt\")\nencoded_test_inputs = tokenizer(xtest, padding=True, truncation=True, return_tensors=\"pt\")\ntrain_input_ids = encoded_train_inputs[\"input_ids\"]\ntrain_attention_mask = encoded_train_inputs[\"attention_mask\"]\ntest_input_ids = encoded_test_inputs[\"input_ids\"]\ntest_attention_mask = encoded_test_inputs[\"attention_mask\"]\n\n# Convert labels to tensors\nytrain_tensor = torch.tensor(ytrain)\nytest_tensor = torch.tensor(ytest)\n\n# Create DataLoader for training and testing\nclass_weights = 1.0 / np.bincount(ytrain)\nclass_weights = torch.FloatTensor(class_weights)\n\ntrain_dataset = TensorDataset(train_input_ids, train_attention_mask, ytrain_tensor)\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, ytest_tensor)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\naccelerator = Accelerator()\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", ncols=100)\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        \n    progress_bar.set_postfix({\"Loss\": total_loss / len(progress_bar)})\n        \n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n\n# Evaluation\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids_batch, attention_mask_batch, _ = batch  # Labels are not needed for inference\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1)\n        all_predictions.extend(predicted_labels.cpu().numpy())\n\n# Calculate accuracy on the test set\ntest_accuracy = accuracy_score(ytest, all_predictions)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GPU","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Load data from CSV\n# data = pd.read_csv(\"your_data.csv\")  # Replace with your CSV file\n\n# Assuming the CSV has columns \"text\" and \"label\"\ntexts = df[\"Summary\"].tolist()\nlabels = df[\"binary\"].tolist()  # Make sure the label column is encoded as integers (0, 1, 2)\n\n# Split data into train and test sets\nxtrain, xtest, ytrain, ytest = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 sentiment classes\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Tokenize the input data\nencoded_train_inputs = tokenizer(xtrain, padding=True, truncation=True, return_tensors=\"pt\")\nencoded_test_inputs = tokenizer(xtest, padding=True, truncation=True, return_tensors=\"pt\")\ntrain_input_ids = encoded_train_inputs[\"input_ids\"].to(device)  # Move to the same device as the model\ntrain_attention_mask = encoded_train_inputs[\"attention_mask\"].to(device)  # Move to the same device as the model\ntest_input_ids = encoded_test_inputs[\"input_ids\"].to(device)  # Move to the same device as the model\ntest_attention_mask = encoded_test_inputs[\"attention_mask\"].to(device)  # Move to the same device as the model\n\n# Convert labels to tensors and move to the same device\nytrain_tensor = torch.tensor(ytrain, device=device)\nytest_tensor = torch.tensor(ytest, device=device)\n\n# Create DataLoader for training and testing\nclass_weights = 1.0 / np.bincount(ytrain)\nclass_weights = torch.FloatTensor(class_weights)\n\ntrain_dataset = TensorDataset(train_input_ids, train_attention_mask, ytrain_tensor)\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, ytest_tensor)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n# accelerator = Accelerator()\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", ncols=100)\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        \n    progress_bar.set_postfix({\"Loss\": total_loss / len(progress_bar)})\n        \n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n\n# Evaluation\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids_batch, attention_mask_batch, _ = batch  # Labels are not needed for inference\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1)\n        all_predictions.extend(predicted_labels.cpu().numpy())\n\n# Calculate accuracy on the test set\ntest_accuracy = accuracy_score(ytest, all_predictions)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:03:25.405849Z","iopub.execute_input":"2023-08-25T08:03:25.406211Z","iopub.status.idle":"2023-08-25T08:03:28.578989Z","shell.execute_reply.started":"2023-08-25T08:03:25.406182Z","shell.execute_reply":"2023-08-25T08:03:28.577688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GPU-5lakh","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Load data from CSV\n# data = pd.read_csv(\"your_data.csv\")  # Replace with your CSV file\n\n# Assuming the CSV has columns \"text\" and \"label\"\ntexts = df[\"Summary\"].tolist()\nlabels = df[\"binary\"].tolist()  # Make sure the label column is encoded as integers (0, 1, 2)\n\n# Split data into train and test sets\nxtrain, xtest, ytrain, ytest = train_test_split(texts, labels, test_size=0.2, random_state=42)\n# Split data into train, validation, and test sets\nxtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.1, random_state=42)  # Split again for validation\n\n# ... (rest of your code)\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 sentiment classes\n\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n# tokenizer.to(device)\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")  # Use the first GPU\n    model = model.to(device)\n    model = nn.DataParallel(model, device_ids=[0, 1])  # Use both GPUs (cuda:0 and cuda:1)\nmodel.to(device)\n\nytrain_tensor = torch.tensor(ytrain, device=device)\nytest_tensor = torch.tensor(ytest, device=device)\n\n# Create DataLoader for training and testing\nclass_weights = 1.0 / np.bincount(ytrain)\nclass_weights = torch.FloatTensor(class_weights)\n# Tokenize the input data\nfrom torch.utils.data import DataLoader, Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n        \n        input_ids = encoding['input_ids'].squeeze().to(device)\n        attention_mask = encoding['attention_mask'].squeeze().to(device)\n        \n        return input_ids, attention_mask, label\n\ntrain_dataset = CustomDataset(xtrain, ytrain, tokenizer, max_length=128)  # Adjust max_length as needed\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nval_dataset = CustomDataset(xval, yval, tokenizer, max_length=128)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n# Similar DataLoader setup for test data\ntest_dataset = CustomDataset(xtest, ytest, tokenizer, max_length=128)  # Adjust max_length as needed\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Convert labels to tensors and move to the same device\n\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n# accelerator = Accelerator()\ngradient_accumulation_steps = 4  # Accumulate gradients over 4 steps\n\n\ntrain_losses = []\ntrain_accuracies = []\nvalidation_losses = []\nvalidation_accuracies = []\ntrain_predictions = []\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n#     train_accuracies = []\n#     progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", ncols=100)\n    #progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{5}', dynamic_ncols=True)\n    start_time = time.time()\n    train_predictions = []\n    for step, batch in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        \n        y_batch = y_batch.to(input_ids_batch.device)\n        \n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()  # Convert to GPU-compatible list\n    \n        train_predictions.extend(predicted_labels)\n        loss = loss_fn(outputs.logits, y_batch)\n#         loss = outputs.loss\n        loss = loss.mean()  \n        loss = loss / gradient_accumulation_steps  # Normalize the loss\n        loss.backward()\n        if (step + 1) % gradient_accumulation_steps == 0:\n            optimizer.step()\n            total_loss += loss.item()\n        elapsed_time = time.time() - start_time\n        remaining_steps = len(train_dataloader) - step\n        remaining_time = elapsed_time * remaining_steps\n#         train_predictions.extend(predicted_labels.numpy())  # Extend the list of predictions\n        batch_description = f'Epoch {epoch + 1}/{5}, Batch {step + 1}/{len(train_dataloader)}, Remaining Time: {remaining_time:.2f} sec'\n        print(batch_description, end='\\r') \n    print()\n    total_loss /= (step + 1)\n    train_losses.append(total_loss)\n    train_accuracies.append(accuracy_score(ytrain, train_predictions))\n\n    #progress_bar.set_postfix({\"Remaining Time\": f\"{remaining_time:.2f} sec\"})\n#     progress_bar.close()\n    # Training complete for this epoch\n    \n    \n    model.eval()  # Set the model to evaluation mode\n    val_loss = 0\n    val_predictions = []\n    #validation_accuracies = []\n    with torch.no_grad():\n        for step, batch in enumerate(val_dataloader):\n            input_ids_batch, attention_mask_batch, y_batch = batch\n            outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n            predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()  # Convert to GPU-compatible list\n    \n            val_predictions.extend(predicted_labels)\n            y_batch = y_batch.to(outputs.logits.device)  # Move labels to the same device as the model output\n            loss = loss_fn(outputs.logits, y_batch)\n            val_loss += loss.mean().item()\n            batch_description1 = f'Epoch {epoch + 1}/{5}, Batch {step + 1}/{len(val_dataloader)}, Remaining Time: {remaining_time:.2f} sec'\n            print(batch_description1, end='\\r') \n    print()\n    val_loss /= len(val_dataloader)\n    validation_losses.append(val_loss)\n    validation_accuracies.append(accuracy_score(yval, val_predictions))\n    #print(f\"Epoch {epoch + 1}, Train Loss: {total_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n    \n#     progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{5}', dynamic_ncols=True)\n    print(f\"Epoch {epoch + 1}, Train Loss: {train_losses[-1]}, Train Acc: {train_accuracies[-1]}, Validation Loss: {validation_losses[-1]}, Validation Acc: {validation_accuracies[-1]}\")\n    \n    #print(f\"Epoch {epoch + 1} training finished\")\n#     progress_bar.set_postfix({\"Loss\": total_loss / (step + 1)})\n        \n      \n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n# Evaluation\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids_batch, attention_mask_batch, _ = batch  # Labels are not needed for inference\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1)\n        all_predictions.extend(predicted_labels.cpu().numpy())\n\n# Calculate accuracy on the test set\ntest_accuracy = accuracy_score(ytest, all_predictions)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:23:18.590980Z","iopub.execute_input":"2023-08-25T08:23:18.591322Z","iopub.status.idle":"2023-08-25T09:05:09.670593Z","shell.execute_reply.started":"2023-08-25T08:23:18.591294Z","shell.execute_reply":"2023-08-25T09:05:09.669524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# better","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport time\nfrom sklearn.utils.class_weight import compute_class_weight\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Load data from CSV\n# data = pd.read_csv(\"your_data.csv\")  # Replace with your CSV file\n\n# Assuming the CSV has columns \"text\" and \"label\"\ntexts = df[\"Summary\"].tolist()\nlabels = df[\"binary\"].tolist()  # Make sure the label column is encoded as integers (0, 1, 2)\n\n# Split data into train and test sets\nxtrain, xtest, ytrain, ytest = train_test_split(texts, labels, test_size=0.2, random_state=42)\n# Split data into train, validation, and test sets\nxtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.1, random_state=42)  # Split again for validation\n\n# ... (rest of your code)\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 sentiment classes\n\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n# tokenizer.to(device)\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")  # Use the first GPU\n    model = model.to(device)\n    model = nn.DataParallel(model, device_ids=[0, 1])  # Use both GPUs (cuda:0 and cuda:1)\nmodel.to(device)\n\nytrain_tensor = torch.tensor(ytrain, device=device)\nytest_tensor = torch.tensor(ytest, device=device)\n\n# Create DataLoader for training and testing\n# class_weights = 1.0 / np.bincount(ytrain)\n# class_weights = torch.FloatTensor(class_weights)\nclasses = [0, 1, 2]\nclass_weights = compute_class_weight(class_weight='balanced', classes=classes, y=ytrain)\nclass_weights = torch.FloatTensor(class_weights).to(device)\n# Tokenize the input data\nfrom torch.utils.data import DataLoader, Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n        \n        input_ids = encoding['input_ids'].squeeze().to(device)\n        attention_mask = encoding['attention_mask'].squeeze().to(device)\n        \n        return input_ids, attention_mask, label\n\ntrain_dataset = CustomDataset(xtrain, ytrain, tokenizer, max_length=128)  # Adjust max_length as needed\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nval_dataset = CustomDataset(xval, yval, tokenizer, max_length=128)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n# Similar DataLoader setup for test data\ntest_dataset = CustomDataset(xtest, ytest, tokenizer, max_length=128)  # Adjust max_length as needed\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Convert labels to tensors and move to the same device\n\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n# accelerator = Accelerator()\ngradient_accumulation_steps = 4  # Accumulate gradients over 4 steps\n\n\ntrain_losses = []\ntrain_accuracies = []\nvalidation_losses = []\nvalidation_accuracies = []\ntrain_predictions = []\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n#     train_accuracies = []\n#     progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", ncols=100)\n    #progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{5}', dynamic_ncols=True)\n    start_time = time.time()\n    train_predictions = []\n    for step, batch in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        \n        y_batch = y_batch.to(input_ids_batch.device)\n        \n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()  # Convert to GPU-compatible list\n    \n        train_predictions.extend(predicted_labels)\n        loss = loss_fn(outputs.logits, y_batch)\n#         loss = outputs.loss\n        loss = loss.mean()  \n        loss = loss / gradient_accumulation_steps  # Normalize the loss\n        loss.backward()\n        if (step + 1) % gradient_accumulation_steps == 0:\n            optimizer.step()\n            total_loss += loss.item()\n        elapsed_time = time.time() - start_time\n        remaining_steps = len(train_dataloader) - step\n        remaining_time = elapsed_time * remaining_steps\n#         train_predictions.extend(predicted_labels.numpy())  # Extend the list of predictions\n        batch_description = f'Epoch {epoch + 1}/{5}, Batch {step + 1}/{len(train_dataloader)}, Remaining Time: {remaining_time:.2f} sec'\n        print(batch_description, end='\\r') \n    print()\n    total_loss /= (step + 1)\n    train_losses.append(total_loss)\n    train_accuracies.append(accuracy_score(ytrain, train_predictions))\n\n    #progress_bar.set_postfix({\"Remaining Time\": f\"{remaining_time:.2f} sec\"})\n#     progress_bar.close()\n    # Training complete for this epoch\n    \n    \n    model.eval()  # Set the model to evaluation mode\n    val_loss = 0\n    val_predictions = []\n    #validation_accuracies = []\n    with torch.no_grad():\n        for step, batch in enumerate(val_dataloader):\n            input_ids_batch, attention_mask_batch, y_batch = batch\n            outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n            predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()  # Convert to GPU-compatible list\n    \n            val_predictions.extend(predicted_labels)\n            y_batch = y_batch.to(outputs.logits.device)  # Move labels to the same device as the model output\n            loss = loss_fn(outputs.logits, y_batch)\n            val_loss += loss.mean().item()\n            batch_description1 = f'Epoch {epoch + 1}/{5}, Batch {step + 1}/{len(val_dataloader)}, Remaining Time: {remaining_time:.2f} sec'\n            print(batch_description1, end='\\r') \n    print()\n    val_loss /= len(val_dataloader)\n    validation_losses.append(val_loss)\n    validation_accuracies.append(accuracy_score(yval, val_predictions))\n    #print(f\"Epoch {epoch + 1}, Train Loss: {total_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n    \n#     progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{5}', dynamic_ncols=True)\n    print(f\"Epoch {epoch + 1}, Train Loss: {train_losses[-1]}, Train Acc: {train_accuracies[-1]}, Validation Loss: {validation_losses[-1]}, Validation Acc: {validation_accuracies[-1]}\")\n    \n    #print(f\"Epoch {epoch + 1} training finished\")\n#     progress_bar.set_postfix({\"Loss\": total_loss / (step + 1)})\n        \n      \n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n# Evaluation\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids_batch, attention_mask_batch, _ = batch  # Labels are not needed for inference\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1)\n        all_predictions.extend(predicted_labels.cpu().numpy())\n\n# Calculate accuracy on the test set\ntest_accuracy = accuracy_score(ytest, all_predictions)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T09:10:01.004151Z","iopub.execute_input":"2023-08-25T09:10:01.004576Z","iopub.status.idle":"2023-08-25T09:51:12.680050Z","shell.execute_reply.started":"2023-08-25T09:10:01.004541Z","shell.execute_reply":"2023-08-25T09:51:12.678987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-08-26T06:46:45.703483Z","iopub.execute_input":"2023-08-26T06:46:45.703856Z","iopub.status.idle":"2023-08-26T06:46:45.727861Z","shell.execute_reply.started":"2023-08-26T06:46:45.703826Z","shell.execute_reply":"2023-08-26T06:46:45.726430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=ytrain)\nclass_weights","metadata":{"execution":{"iopub.status.busy":"2023-08-26T07:19:11.262438Z","iopub.execute_input":"2023-08-26T07:19:11.263367Z","iopub.status.idle":"2023-08-26T07:19:11.278471Z","shell.execute_reply.started":"2023-08-26T07:19:11.263331Z","shell.execute_reply":"2023-08-26T07:19:11.277397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Last","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport time\nfrom sklearn.utils.class_weight import compute_class_weight\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Load data from CSV\n# data = pd.read_csv(\"your_data.csv\")  # Replace with your CSV file\n\n# Assuming the CSV has columns \"text\" and \"label\"\ntexts = df[\"Text\"].tolist()\nlabels = df[\"binary\"].tolist()  # Make sure the label column is encoded as integers (0, 1, 2)\n\n# Split data into train and test sets\nxtrain, xtest, ytrain, ytest = train_test_split(texts, labels, test_size=0.2, random_state=42)\n# Split data into train, validation, and test sets\nxtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.1, random_state=42)  # Split again for validation\n\n# ... (rest of your code)\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 3 sentiment classes\n\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n# tokenizer.to(device)\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")  # Use the first GPU\n    model = model.to(device)\n    model = nn.DataParallel(model, device_ids=[0, 1])  # Use both GPUs (cuda:0 and cuda:1)\nmodel.to(device)\n\nytrain_tensor = torch.tensor(ytrain, device=device)\nytest_tensor = torch.tensor(ytest, device=device)\n\n# Create DataLoader for training and testing\n# class_weights = 1.0 / np.bincount(ytrain)\n# class_weights = torch.FloatTensor(class_weights)\nclasses = [0, 1]\nclass_weights = compute_class_weight(class_weight='balanced', classes=classes, y=ytrain)\n\n# class_weights = compute_class_weight(class_weight=None, classes=classes, y=ytrain)\n# custom_weights=np.array([1, 1])  \n# class_weights = class_weights * custom_weights\nclass_weights = torch.FloatTensor(class_weights).to(device)\n# Tokenize the input data\nfrom torch.utils.data import DataLoader, Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n        \n        input_ids = encoding['input_ids'].squeeze().to(device)\n        attention_mask = encoding['attention_mask'].squeeze().to(device)\n        \n        return input_ids, attention_mask, label\n\ntrain_dataset = CustomDataset(xtrain, ytrain, tokenizer, max_length=128)  # Adjust max_length as needed\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nval_dataset = CustomDataset(xval, yval, tokenizer, max_length=128)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n# Similar DataLoader setup for test data\ntest_dataset = CustomDataset(xtest, ytest, tokenizer, max_length=128)  # Adjust max_length as needed\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Convert labels to tensors and move to the same device\n\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n#loss_fn = torch.nn.CrossEntropyLoss()\n# accelerator = Accelerator()\ngradient_accumulation_steps = 4  # Accumulate gradients over 4 steps\n\n\ntrain_losses = []\ntrain_accuracies = []\nvalidation_losses = []\nvalidation_accuracies = []\ntrain_predictions = []\nfor epoch in range(3):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n#     train_accuracies = []\n#     progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", ncols=100)\n    #progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{5}', dynamic_ncols=True)\n    start_time = time.time()\n    train_predictions = []\n    for step, batch in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        \n        y_batch = y_batch.to(input_ids_batch.device)\n        \n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()  # Convert to GPU-compatible list\n    \n        train_predictions.extend(predicted_labels)\n        loss = loss_fn(outputs.logits, y_batch)\n#         loss = outputs.loss\n        loss = loss.mean()  \n        loss = loss / gradient_accumulation_steps  # Normalize the loss\n        loss.backward()\n        if (step + 1) % gradient_accumulation_steps == 0:\n            optimizer.step()\n            total_loss += loss.item()\n        elapsed_time = time.time() - start_time\n#         remaining_steps = len(train_dataloader) - step\n#         remaining_time = elapsed_time * remaining_steps\n#         train_predictions.extend(predicted_labels.numpy())  # Extend the list of predictions\n        batch_description = f'Epoch {epoch + 1}/{3}, Batch {step + 1}/{len(train_dataloader)}, Elapsed Time: {elapsed_time:.2f} sec'\n        print(batch_description, end='\\r') \n    print()\n    total_loss /= (step + 1)\n    train_losses.append(total_loss)\n    train_accuracies.append(accuracy_score(ytrain, train_predictions))\n\n    #progress_bar.set_postfix({\"Remaining Time\": f\"{remaining_time:.2f} sec\"})\n#     progress_bar.close()\n    # Training complete for this epoch\n    \n    \n    model.eval()  # Set the model to evaluation mode\n    val_loss = 0\n    val_predictions = []\n    #validation_accuracies = []\n    start_time = time.time()\n    with torch.no_grad():\n        for step, batch in enumerate(val_dataloader):\n            input_ids_batch, attention_mask_batch, y_batch = batch\n            outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n            predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()  # Convert to GPU-compatible list\n    \n            val_predictions.extend(predicted_labels)\n            y_batch = y_batch.to(outputs.logits.device)  # Move labels to the same device as the model output\n            loss = loss_fn(outputs.logits, y_batch)\n            val_loss += loss.mean().item()\n            elapsed_time = time.time() - start_time\n            \n            batch_description1 = f'Epoch {epoch + 1}/{3}, Batch {step + 1}/{len(val_dataloader)}, Elapsed Time: {elapsed_time:.2f} sec'\n            print(batch_description1, end='\\r') \n    print()\n    val_loss /= len(val_dataloader)\n    validation_losses.append(val_loss)\n    validation_accuracies.append(accuracy_score(yval, val_predictions))\n    #print(f\"Epoch {epoch + 1}, Train Loss: {total_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n    \n#     progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{5}', dynamic_ncols=True)\n    print(f\"Epoch {epoch + 1}, Train Loss: {train_losses[-1]}, Train Acc: {train_accuracies[-1]}, Validation Loss: {validation_losses[-1]}, Validation Acc: {validation_accuracies[-1]}\")\n    \n    #print(f\"Epoch {epoch + 1} training finished\")\n#     progress_bar.set_postfix({\"Loss\": total_loss / (step + 1)})\n        \n      \n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n# Evaluation\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids_batch, attention_mask_batch, _ = batch  # Labels are not needed for inference\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1)\n        all_predictions.extend(predicted_labels.cpu().numpy())\n\n# Calculate accuracy on the test set\ntest_accuracy = accuracy_score(ytest, all_predictions)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:55:03.849621Z","iopub.execute_input":"2023-08-31T12:55:03.850034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"7.05\n7.31\n7.56\n8.15\n8.31\n8.58\n9.26\n9.55\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gff","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\n\n# Plot Losses\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(validation_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\n\n# Plot Accuracies\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label='Training Accuracy')\nplt.plot(validation_accuracies, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Training and Validation Accuracy')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T09:56:43.159772Z","iopub.execute_input":"2023-08-25T09:56:43.160145Z","iopub.status.idle":"2023-08-25T09:56:43.835104Z","shell.execute_reply.started":"2023-08-25T09:56:43.160115Z","shell.execute_reply":"2023-08-25T09:56:43.834143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n# Example true labels and predicted labels\n\n\n# Compute confusion matrix\ncm = confusion_matrix(ytest, all_predictions)\n\n# Create a DataFrame from the confusion matrix\nconfusion_matrix_df = pd.DataFrame(cm, index=['0', '1', '2'], columns=['0', '1', '2'])\n\n# Set color palette for the heatmap\n#cmap = sns.color_palette(\"Blues\")\nbase_color = \"#3D59AB\"  # Specify the base color\nnum_shades = 920  # Specify the number of shades you want\n\n# Create a list of shades of the base color\nshades = sns.light_palette(base_color, n_colors=num_shades)\n\n# Create a custom color map\ncmap = sns.color_palette(shades)\n\n# Plot the confusion matrix using a heatmap\nsns.heatmap(confusion_matrix_df, annot=True, cmap='Blues', fmt='')\n#annot_kws={\"size\": 10}\n# Add labels and title\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T09:54:33.411746Z","iopub.execute_input":"2023-08-25T09:54:33.412155Z","iopub.status.idle":"2023-08-25T09:54:34.064310Z","shell.execute_reply.started":"2023-08-25T09:54:33.412123Z","shell.execute_reply":"2023-08-25T09:54:34.063379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\n\n# Load data from CSV\n# data = pd.read_csv(\"your_data.csv\")  # Replace with your CSV file\n\n# Assuming the CSV has columns \"text\" and \"label\"\ntexts = df[\"Text\"].tolist()\nlabels = df[\"binary\"].tolist()  # Make sure the label column is encoded as integers (0, 1, 2)\n\n# Split data into train and test sets\nxtrain, xtest, ytrain, ytest = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 3 sentiment classes\n\n# Check if GPUs are available\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")  # Use the first GPU\n    model = model.to(device)\n    model = nn.DataParallel(model, device_ids=[0, 1])  # Use both GPUs (cuda:0 and cuda:1)\n\nytrain_tensor = torch.tensor(ytrain, device=device)\nytest_tensor = torch.tensor(ytest, device=device)\n\n# Create DataLoader for training and testing\nclass_weights = 1.0 / np.bincount(ytrain)\nclass_weights = torch.FloatTensor(class_weights)\n\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n        \n        input_ids = encoding['input_ids'].squeeze().to(device)\n        attention_mask = encoding['attention_mask'].squeeze().to(device)\n        \n        return input_ids, attention_mask, label\n\ntrain_dataset = CustomDataset(xtrain, ytrain_tensor, tokenizer, max_length=128)  # Adjust max_length as needed\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Similar DataLoader setup for test data\ntest_dataset = CustomDataset(xtest, ytest_tensor, tokenizer, max_length=128)  # Adjust max_length as needed\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\ngradient_accumulation_steps = 4  # Accumulate gradients over 4 steps\n\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{5}', dynamic_ncols=True)\n    start_time = time.time()\n    for step, batch in enumerate(progress_bar):\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        loss = outputs.loss\n        loss = loss / gradient_accumulation_steps  # Normalize the loss\n        loss.backward()\n        if (step + 1) % gradient_accumulation_steps == 0:\n            optimizer.step()\n            total_loss += loss.item()\n        elapsed_time = time.time() - start_time\n        remaining_steps = len(train_dataloader) - step\n        remaining_time = elapsed_time * remaining_steps\n        progress_bar.set_postfix({\"Remaining Time\": f\"{remaining_time:.2f} sec\"})\n\n    # Training complete for this epoch\n    print(f\"Epoch {epoch + 1} training finished\")\n        \n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n\n# Evaluation\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids_batch, attention_mask_batch, _ = batch  # Labels are not needed for inference\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1)\n        all_predictions.extend(predicted_labels.cpu().numpy())\n\n# Calculate accuracy on the test set\ntest_accuracy = accuracy_score(ytest, all_predictions)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:40:27.965478Z","iopub.execute_input":"2023-08-25T06:40:27.965858Z","iopub.status.idle":"2023-08-25T06:40:30.410805Z","shell.execute_reply.started":"2023-08-25T06:40:27.965828Z","shell.execute_reply":"2023-08-25T06:40:30.409498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Past","metadata":{}},{"cell_type":"code","source":"jgjgjgj","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load data from CSV\n# data = pd.read_csv(\"your_data.csv\")  # Replace with your CSV file\n\n# Assuming the CSV has columns \"text\" and \"label\"\ntexts = df[\"Summary\"].tolist()\nlabels = df[\"binary\"].tolist()  # Make sure the label column is encoded as integers (0, 1, 2)\n\n# Split data into train and test sets\nxtrain, xtest, ytrain, ytest = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 sentiment classes\n\n# Tokenize the input data\nencoded_train_inputs = tokenizer(xtrain, padding=True, truncation=True, return_tensors=\"pt\")\nencoded_test_inputs = tokenizer(xtest, padding=True, truncation=True, return_tensors=\"pt\")\ntrain_input_ids = encoded_train_inputs[\"input_ids\"]\ntrain_attention_mask = encoded_train_inputs[\"attention_mask\"]\ntest_input_ids = encoded_test_inputs[\"input_ids\"]\ntest_attention_mask = encoded_test_inputs[\"attention_mask\"]\n\n# Convert labels to tensors\nytrain_tensor = torch.tensor(ytrain)\nytest_tensor = torch.tensor(ytest)\n\n# Create DataLoader for training and testing\ntrain_dataset = TensorDataset(train_input_ids, train_attention_mask, ytrain_tensor)\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, ytest_tensor)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n\n# Evaluation\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids_batch, attention_mask_batch, _ = batch  # Labels are not needed for inference\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1)\n        all_predictions.extend(predicted_labels.cpu().numpy())\n\n# Calculate accuracy on the test set\ntest_accuracy = accuracy_score(ytest, all_predictions)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T17:58:22.366401Z","iopub.execute_input":"2023-08-24T17:58:22.366806Z","iopub.status.idle":"2023-08-24T18:15:00.662255Z","shell.execute_reply.started":"2023-08-24T17:58:22.366744Z","shell.execute_reply":"2023-08-24T18:15:00.661123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accumulation_steps = 4  # Accumulate gradients over 4 batches\n\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    accumulated_loss = 0\n    \n    for i, batch in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        loss = outputs.loss\n        \n        # Accumulate gradients\n        loss = loss / accumulation_steps\n        accumulated_loss += loss\n        \n        if (i + 1) % accumulation_steps == 0 or i == len(train_dataloader) - 1:\n            accumulated_loss.backward()\n            optimizer.step()\n            total_loss += accumulated_loss.item()\n            accumulated_loss = 0\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids_batch, attention_mask_batch, _ = batch  # Labels are not needed for inference\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1)\n        all_predictions.extend(predicted_labels.cpu().numpy())\n\n# Calculate accuracy on the test set\ntest_accuracy = accuracy_score(ytest, all_predictions)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load data from CSV\n# data = pd.read_csv(\"your_data.csv\")  # Replace with your CSV file\n\n# Assuming the CSV has columns \"text\" and \"label\"\ntexts = df[\"Summary\"].tolist()\nlabels = df[\"binary\"].tolist()  # Make sure the label column is encoded as integers (0, 1, 2)\n\n# Split data into train and test sets\nxtrain, xtest, ytrain, ytest = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 sentiment classes\n\n\n\nencoded_train_inputs = tokenizer(xtrain, padding=True, truncation=True, return_tensors=\"pt\")\nencoded_test_inputs = tokenizer(xtest, padding=True, truncation=True, return_tensors=\"pt\")\ntrain_input_ids = encoded_train_inputs[\"input_ids\"]\ntrain_attention_mask = encoded_train_inputs[\"attention_mask\"]\ntest_input_ids = encoded_test_inputs[\"input_ids\"]\ntest_attention_mask = encoded_test_inputs[\"attention_mask\"]\n\n# Convert labels to tensors\nytrain_tensor = torch.tensor(ytrain)\nytest_tensor = torch.tensor(ytest)\n\n# Create DataLoader for training and testing\ntrain_dataset = TensorDataset(train_input_ids, train_attention_mask, ytrain_tensor)\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, ytest_tensor)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# ... (Rest of your data processing and training loop)\n\n# Evaluation\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids_batch, attention_mask_batch, _ = batch  # Labels are not needed for inference\n        input_ids_batch = input_ids_batch.to(device)\n        attention_mask_batch = attention_mask_batch.to(device)\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1)\n        all_predictions.extend(predicted_labels.cpu().numpy())\n\n# Calculate accuracy on the test set\ntest_accuracy = accuracy_score(ytest, all_predictions)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load data from CSV\n# data = pd.read_csv(\"your_data.csv\")  # Replace with your CSV file\n\n# ... (Rest of the code remains unchanged up to the DataLoader setup)\n\n# Compute class weights for reweighting\nclass_weights = 1.0 / np.bincount(ytrain)\nclass_weights = torch.FloatTensor(class_weights)\n\n# Create DataLoader for training and testing\ntrain_dataset = TensorDataset(train_input_ids, train_attention_mask, ytrain_tensor)\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, ytest_tensor)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# ... (Rest of the code remains unchanged for the training loop and evaluation)\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)  # Apply class weights\n\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n\n# Evaluation\n# ... (Rest of the code remains unchanged for the evaluation)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load data from CSV\n# data = pd.read_csv(\"your_data.csv\")  # Replace with your CSV file\n\n# Assuming the CSV has columns \"text\" and \"label\"\ntexts = df[\"Summary\"].tolist()\nlabels = df[\"binary\"].tolist()  # Make sure the label column is encoded as integers (0, 1, 2)\n\n# Split data into train and test sets\nxtrain, xtest, ytrain, ytest = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 sentiment classes\n\n# Tokenize the input data\nencoded_train_inputs = tokenizer(xtrain, padding=True, truncation=True, return_tensors=\"pt\")\nencoded_test_inputs = tokenizer(xtest, padding=True, truncation=True, return_tensors=\"pt\")\ntrain_input_ids = encoded_train_inputs[\"input_ids\"]\ntrain_attention_mask = encoded_train_inputs[\"attention_mask\"]\ntest_input_ids = encoded_test_inputs[\"input_ids\"]\ntest_attention_mask = encoded_test_inputs[\"attention_mask\"]\n\n# Convert labels to tensors\nytrain_tensor = torch.tensor(ytrain)\nytest_tensor = torch.tensor(ytest)\n\n# Create DataLoader for training and testing\nclass_weights = 1.0 / np.bincount(ytrain)\nclass_weights = torch.FloatTensor(class_weights)\n\ntrain_dataset = TensorDataset(train_input_ids, train_attention_mask, ytrain_tensor)\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, ytest_tensor)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n\n# Evaluation\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids_batch, attention_mask_batch, _ = batch  # Labels are not needed for inference\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n        predicted_labels = torch.argmax(outputs.logits, dim=1)\n        all_predictions.extend(predicted_labels.cpu().numpy())\n\n# Calculate accuracy on the test set\ntest_accuracy = accuracy_score(ytest, all_predictions)\nprint(\"Test Accuracy:\", test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T18:17:39.971588Z","iopub.execute_input":"2023-08-24T18:17:39.972673Z","iopub.status.idle":"2023-08-24T18:33:44.197849Z","shell.execute_reply.started":"2023-08-24T18:17:39.972635Z","shell.execute_reply":"2023-08-24T18:33:44.196708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=train.iloc[:1000]\ntest=test.iloc[:1000]\n# sub=sub.iloc[:10000]\n# sub=sub.iloc[-10000:]\n# sub","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:51:42.332114Z","iopub.execute_input":"2023-08-20T10:51:42.332619Z","iopub.status.idle":"2023-08-20T10:51:42.343497Z","shell.execute_reply.started":"2023-08-20T10:51:42.332564Z","shell.execute_reply":"2023-08-20T10:51:42.341670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=df[\"Score\"].values\nimport seaborn as sns\nsns.displot(y)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T11:20:57.702021Z","iopub.execute_input":"2023-08-24T11:20:57.702633Z","iopub.status.idle":"2023-08-24T11:20:59.209516Z","shell.execute_reply.started":"2023-08-24T11:20:57.702576Z","shell.execute_reply":"2023-08-24T11:20:59.208405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n\n# Adding a new column based on existing column values using custom function\ndef custom_transformation(age):\n    if age < 4:\n        return 0\n    else:\n        return 1\n\ndf['binary'] = df['Score'].apply(custom_transformation)\n\nprint(df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = 'ternary.csv'\ndf.to_csv(output_file, index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T17:57:49.453841Z","iopub.execute_input":"2023-08-24T17:57:49.454203Z","iopub.status.idle":"2023-08-24T17:57:49.529024Z","shell.execute_reply.started":"2023-08-24T17:57:49.454174Z","shell.execute_reply":"2023-08-24T17:57:49.528011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Start","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the CSV data into a DataFrame\n# data = pd.read_csv('/kaggle/input/amazon-fine-food-reviews/Reviews.csv').iloc[:1000]\n# data = pd.read_csv('/kaggle/working/ternary.csv')\n# .iloc[:50000]\ndata = df\n\n# Define your features (X) and target variable (y)\nX = data\n# y = data['Score']\ny = data['binary']\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T11:21:45.342514Z","iopub.execute_input":"2023-08-24T11:21:45.343102Z","iopub.status.idle":"2023-08-24T11:21:45.491463Z","shell.execute_reply.started":"2023-08-24T11:21:45.343068Z","shell.execute_reply":"2023-08-24T11:21:45.490305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=X_train[[\"Summary\",\"binary\"]]\ntrain","metadata":{"execution":{"iopub.status.busy":"2023-08-24T11:21:55.152411Z","iopub.execute_input":"2023-08-24T11:21:55.152953Z","iopub.status.idle":"2023-08-24T11:21:55.172195Z","shell.execute_reply.started":"2023-08-24T11:21:55.152918Z","shell.execute_reply":"2023-08-24T11:21:55.170677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ytrain=X_train[[\"binary\"]]\nytrain","metadata":{"execution":{"iopub.status.busy":"2023-08-24T11:24:10.780804Z","iopub.execute_input":"2023-08-24T11:24:10.781177Z","iopub.status.idle":"2023-08-24T11:24:10.794064Z","shell.execute_reply.started":"2023-08-24T11:24:10.781146Z","shell.execute_reply":"2023-08-24T11:24:10.792860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=train[['Summary']]\ntrain","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-24T11:23:28.243366Z","iopub.execute_input":"2023-08-24T11:23:28.243737Z","iopub.status.idle":"2023-08-24T11:23:28.256975Z","shell.execute_reply.started":"2023-08-24T11:23:28.243698Z","shell.execute_reply":"2023-08-24T11:23:28.255674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text=X_test[\"Summary\"]\ntest_text","metadata":{"execution":{"iopub.status.busy":"2023-08-24T11:22:23.109179Z","iopub.execute_input":"2023-08-24T11:22:23.110221Z","iopub.status.idle":"2023-08-24T11:22:23.119595Z","shell.execute_reply.started":"2023-08-24T11:22:23.110186Z","shell.execute_reply":"2023-08-24T11:22:23.118257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain1","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:04:05.085882Z","iopub.execute_input":"2023-08-24T06:04:05.086276Z","iopub.status.idle":"2023-08-24T06:04:05.094551Z","shell.execute_reply.started":"2023-08-24T06:04:05.086246Z","shell.execute_reply":"2023-08-24T06:04:05.093642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Sample data\nxtrain1 = train\nytrain = ytrain\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = \"siebert/sentiment-roberta-large-english\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # Assuming 2 labels\n\n# Tokenize the input data\nencoded_inputs = tokenizer(xtrain1, padding=True, truncation=True, return_tensors=\"pt\")\ninput_ids = encoded_inputs[\"input_ids\"]\nattention_mask = encoded_inputs[\"attention_mask\"]\n\n# Convert labels to tensor\nytrain_tensor = torch.tensor(ytrain)\n\n# Define a DataLoader\ndataset = TensorDataset(input_ids, attention_mask, ytrain_tensor)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    total_loss = 0\n    for batch in dataloader:\n        optimizer.zero_grad()\n        input_ids_batch, attention_mask_batch, y_batch = batch\n        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=y_batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n\n# Inference\nmodel.eval()\ntest_text=test_data\nwith torch.no_grad():\n#     Assuming you have a separate test set, preprocess it similarly\n    encoded_test_inputs = tokenizer(test_data, padding=True, truncation=True, return_tensors=\"pt\")\n    test_input_ids = encoded_test_inputs[\"input_ids\"]\n    test_attention_mask = encoded_test_inputs[\"attention_mask\"]\n\n#     Run predictions on test data\n    test_outputs = model(test_input_ids, attention_mask=test_attention_mask)\n    predicted_labels = torch.argmax(test_outputs.logits, dim=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T11:28:54.102396Z","iopub.execute_input":"2023-08-24T11:28:54.102791Z","iopub.status.idle":"2023-08-24T11:29:00.149574Z","shell.execute_reply.started":"2023-08-24T11:28:54.102757Z","shell.execute_reply":"2023-08-24T11:29:00.148025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\nimport emoji\ndef dataPreprocessing(x):    \n    x = x.lower()\n    x = removeHTML(x)\n    x = emoji.demojize(x, delimiters=(\" \", \" \"))\n    x = re.sub(\"@\\w+\", '',x) # removing mentions (@)\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(r\"[^\\w\\s]\", '',x) # to remove symbols\n    x = re.sub(\"http\\w+\", '',x)\n    x = re.sub(\"\\s[a-z]\\s\", '',x)\n    x = x.strip()\n    return x\ntrain_text1=train[\"Summary\"].apply(lambda x: dataPreprocessing(x))","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:51:17.377225Z","iopub.execute_input":"2023-08-24T05:51:17.377614Z","iopub.status.idle":"2023-08-24T05:51:17.437176Z","shell.execute_reply.started":"2023-08-24T05:51:17.377577Z","shell.execute_reply":"2023-08-24T05:51:17.436252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text1=test_text.apply(lambda x: dataPreprocessing(x))","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:51:21.351175Z","iopub.execute_input":"2023-08-24T05:51:21.351618Z","iopub.status.idle":"2023-08-24T05:51:21.380802Z","shell.execute_reply.started":"2023-08-24T05:51:21.351580Z","shell.execute_reply":"2023-08-24T05:51:21.379780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(torch.cuda.is_available())\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:11:03.702000Z","iopub.execute_input":"2023-08-24T05:11:03.702419Z","iopub.status.idle":"2023-08-24T05:11:08.307516Z","shell.execute_reply.started":"2023-08-24T05:11:03.702384Z","shell.execute_reply":"2023-08-24T05:11:08.306392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n\n# # Check if at least one CUDA-enabled GPU is available\n# if torch.cuda.is_available():\n#     # Determine the number of available GPUs\n#     num_gpus = torch.cuda.device_count()\n#     print(f\"Number of available GPUs: {num_gpus}\")\n\n#     # Select GPUs for training (0 and 1 in this case)\n#     device_ids = [0, 1]  # Specify the GPU indices you want to use\n#     device = torch.device(\"cuda:0\")  # This is just a placeholder since DataParallel will handle device placement\n#     # Uncomment the next line to use both GPUs for a single operation\n#     # device = torch.device(\"cuda\" if len(device_ids) > 1 else \"cuda:0\")\n# else:\n#     device = torch.device(\"cpu\")\n\n# print(\"Selected device:\", device)\n\n# # Example usage of torch.nn.DataParallel\n# # model = YourModel()  # Instantiate your model here\n# # model = torch.nn.DataParallel(model, device_ids=device_ids)\n# # model.to(device)  # Move the model to the selected device\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:09:42.693323Z","iopub.execute_input":"2023-08-20T08:09:42.694036Z","iopub.status.idle":"2023-08-20T08:09:46.015071Z","shell.execute_reply.started":"2023-08-20T08:09:42.693996Z","shell.execute_reply":"2023-08-20T08:09:46.014158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, f1_score\nfrom scipy.sparse import hstack\nstopwords = stopwords.words('english')\nstopwords = stopwords[:116]\nstopwords = stopwords.extend(['d', 'll', 're', 's', 've'])\ndata=pd.concat([train_text1,test_text1],axis=0).reset_index()[\"Summary\"]\ndata","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:51:32.160178Z","iopub.execute_input":"2023-08-24T05:51:32.161081Z","iopub.status.idle":"2023-08-24T05:51:32.174402Z","shell.execute_reply.started":"2023-08-24T05:51:32.161034Z","shell.execute_reply":"2023-08-24T05:51:32.173384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words=stopwords,\n    ngram_range=(1, 1),\n    max_features=10000)\nword_vectorizer.fit(data)\ntrain_word_features = word_vectorizer.transform(train_text1)\ntest_word_features = word_vectorizer.transform(test_text1)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:51:48.218870Z","iopub.execute_input":"2023-08-24T05:51:48.219272Z","iopub.status.idle":"2023-08-24T05:51:48.253876Z","shell.execute_reply.started":"2023-08-24T05:51:48.219240Z","shell.execute_reply":"2023-08-24T05:51:48.252847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain2 = hstack([train_word_features])\nxtest2 = hstack([test_word_features])","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:22:48.286368Z","iopub.execute_input":"2023-08-24T05:22:48.286744Z","iopub.status.idle":"2023-08-24T05:22:48.292782Z","shell.execute_reply.started":"2023-08-24T05:22:48.286714Z","shell.execute_reply":"2023-08-24T05:22:48.291756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:56:48.707476Z","iopub.execute_input":"2023-08-20T10:56:48.707971Z","iopub.status.idle":"2023-08-20T10:56:50.650688Z","shell.execute_reply.started":"2023-08-20T10:56:48.707890Z","shell.execute_reply":"2023-08-20T10:56:50.649558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom transformers import AdamW\nfrom tqdm import tqdm\nclass BertClassificationModel(nn.Module):\n    def __init__(self,hidden_size=768,num_class=3): \n        super(BertClassificationModel, self).__init__()\n        model_name = \"siebert/sentiment-roberta-large-english\"\n        self.tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n        self.bert = RobertaModel.from_pretrained(pretrained_model_name_or_path=model_name)\n\n        for p in self.bert.parameters(): \n                p.requires_grad = False\n        self.fc = nn.Linear(hidden_size, num_class)\n\n    def forward(self, batch_sentences): \n        sentences_tokenizer = self.tokenizer(batch_sentences,\n                                             truncation=True,\n                                             padding=True,\n                                             max_length=512,\n                                             add_special_tokens=True)\n        input_ids=torch.tensor(sentences_tokenizer['input_ids']).to(device) \n        attention_mask=torch.tensor(sentences_tokenizer['attention_mask']).to(device) \n        bert_out=self.bert(input_ids=input_ids,attention_mask=attention_mask) \n\n        last_hidden_state =bert_out[0].to(device) \n        bert_cls_hidden_state=last_hidden_state[:,0,:].to(device) \n        return bert_cls_hidden_state\nmodel=BertClassificationModel()\nmodel=model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:51:57.682015Z","iopub.execute_input":"2023-08-24T05:51:57.682582Z","iopub.status.idle":"2023-08-24T05:52:01.642000Z","shell.execute_reply.started":"2023-08-24T05:51:57.682543Z","shell.execute_reply":"2023-08-24T05:52:01.640978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:46:34.720979Z","iopub.execute_input":"2023-08-20T08:46:34.721378Z","iopub.status.idle":"2023-08-20T08:46:34.727280Z","shell.execute_reply.started":"2023-08-20T08:46:34.721343Z","shell.execute_reply":"2023-08-20T08:46:34.726077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if torch.cuda.is_available():\n#     device = torch.device('cuda')\n#     model = model.to(device)\n#     model = nn.DataParallel(model, device_ids=[0, 1])","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:46:37.066846Z","iopub.execute_input":"2023-08-20T08:46:37.067219Z","iopub.status.idle":"2023-08-20T08:46:42.555327Z","shell.execute_reply.started":"2023-08-20T08:46:37.067186Z","shell.execute_reply":"2023-08-20T08:46:42.554218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if torch.cuda.is_available():\n#     # Determine the number of available GPUs\n#     num_gpus = torch.cuda.device_count()\n#     print(f\"Number of available GPUs: {num_gpus}\")\n\n#     # Select GPUs for training (0 and 1 in this case)\n#     device_ids = [0, 1]  # Specify the GPU indices you want to use\n#     device = torch.device(\"cuda:0\")  # This is just a placeholder since DataParallel will handle device placement\n#     # Uncomment the next line to use both GPUs for a single operation\n#     # device = torch.device(\"cuda\" if len(device_ids) > 1 else \"cuda:0\")\n# else:\n#     device = torch.device(\"cpu\")\n\n# print(\"Selected device:\", device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:58:28.677960Z","iopub.execute_input":"2023-08-20T10:58:28.678390Z","iopub.status.idle":"2023-08-20T10:58:33.579819Z","shell.execute_reply.started":"2023-08-20T10:58:28.678358Z","shell.execute_reply":"2023-08-20T10:58:33.578724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain1=[]\nfor i in tqdm(train_text1):\n    xtrain1.append(model([i]).cpu().detach().numpy())\nxtrain1=np.array(xtrain1).reshape(-1,768)  \ndel train_text1","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:52:11.269603Z","iopub.execute_input":"2023-08-24T05:52:11.269990Z","iopub.status.idle":"2023-08-24T05:52:32.741737Z","shell.execute_reply.started":"2023-08-24T05:52:11.269935Z","shell.execute_reply":"2023-08-24T05:52:32.740268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain1=np.array(xtrain1).reshape(-1,204800)  \ndel train_text1","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:54:45.725647Z","iopub.execute_input":"2023-08-24T05:54:45.726050Z","iopub.status.idle":"2023-08-24T05:54:45.780566Z","shell.execute_reply.started":"2023-08-24T05:54:45.726017Z","shell.execute_reply":"2023-08-24T05:54:45.779019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain1=[]\nfor i in tqdm(train_text1):\n    xtrain1.append(model([i]).cpu().detach().numpy())\nxtrain1=np.array(xtrain1)#.reshape(-1,819200)  \ndel train_text1","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:23:17.260667Z","iopub.execute_input":"2023-08-24T05:23:17.261355Z","iopub.status.idle":"2023-08-24T05:23:39.784962Z","shell.execute_reply.started":"2023-08-24T05:23:17.261309Z","shell.execute_reply":"2023-08-24T05:23:39.783967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain1","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:24:35.724636Z","iopub.execute_input":"2023-08-24T05:24:35.725046Z","iopub.status.idle":"2023-08-24T05:24:35.734497Z","shell.execute_reply.started":"2023-08-24T05:24:35.725012Z","shell.execute_reply":"2023-08-24T05:24:35.733119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nmy_array = xtrain1\n\nwith open('xtrain1.csv', 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['Index', 'Value'])  # Writing header\n    for index, value in enumerate(my_array):\n        writer.writerow([index, value])\n","metadata":{"execution":{"iopub.status.busy":"2023-08-21T07:05:44.597275Z","iopub.execute_input":"2023-08-21T07:05:44.597683Z","iopub.status.idle":"2023-08-21T08:17:32.859219Z","shell.execute_reply.started":"2023-08-21T07:05:44.597648Z","shell.execute_reply":"2023-08-21T08:17:32.858077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xtrain1 = []\n# for i in tqdm(train_text1):\n#     xtrain1.append(model([i]).to(device).detach().cpu().numpy())\n# xtrain1 = np.array(xtrain1).reshape(-1, 768)\n# del train_text1","metadata":{"execution":{"iopub.status.busy":"2023-08-19T21:54:57.997476Z","iopub.status.idle":"2023-08-19T21:54:57.998353Z","shell.execute_reply.started":"2023-08-19T21:54:57.998033Z","shell.execute_reply":"2023-08-19T21:54:57.998059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-08-20T19:49:34.351597Z","iopub.execute_input":"2023-08-20T19:49:34.352702Z","iopub.status.idle":"2023-08-20T19:49:34.358223Z","shell.execute_reply.started":"2023-08-20T19:49:34.352648Z","shell.execute_reply":"2023-08-20T19:49:34.357063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtest1=[]\nfor i in tqdm(test_text1):\n    xtest1.append(model([i]).cpu().detach().numpy())\nxtest1=np.array(xtest1).reshape(-1,768)\ndel test_text1","metadata":{"execution":{"iopub.status.busy":"2023-08-21T06:33:18.171688Z","iopub.execute_input":"2023-08-21T06:33:18.172120Z","iopub.status.idle":"2023-08-21T06:58:21.639126Z","shell.execute_reply.started":"2023-08-21T06:33:18.172085Z","shell.execute_reply":"2023-08-21T06:58:21.638026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtest1=[]\nfor i in tqdm(test_text1):\n    xtest1.append(model([i]).cpu().detach().numpy())\nxtest1=np.array(xtest1).reshape(-1,204800)\ndel test_text1","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:53:15.963402Z","iopub.execute_input":"2023-08-24T05:53:15.963780Z","iopub.status.idle":"2023-08-24T05:53:21.024730Z","shell.execute_reply.started":"2023-08-24T05:53:15.963749Z","shell.execute_reply":"2023-08-24T05:53:21.023537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nmy_array = xtest1\n\nwith open('xtest1.csv', 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['Index', 'Value'])  # Writing header\n    for index, value in enumerate(my_array):\n        writer.writerow([index, value])\n","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:38:18.652865Z","iopub.execute_input":"2023-08-21T08:38:18.653308Z","iopub.status.idle":"2023-08-21T08:56:35.664365Z","shell.execute_reply.started":"2023-08-21T08:38:18.653272Z","shell.execute_reply":"2023-08-21T08:56:35.654932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ytrain=train[\"binary\"].values\nimport seaborn as sns\nsns.displot(ytrain)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:32:27.950759Z","iopub.execute_input":"2023-08-24T05:32:27.951172Z","iopub.status.idle":"2023-08-24T05:32:28.540413Z","shell.execute_reply.started":"2023-08-24T05:32:27.951139Z","shell.execute_reply":"2023-08-24T05:32:28.539221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ytrain.size","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:53:26.737302Z","iopub.execute_input":"2023-08-24T05:53:26.737836Z","iopub.status.idle":"2023-08-24T05:53:26.750770Z","shell.execute_reply.started":"2023-08-24T05:53:26.737803Z","shell.execute_reply":"2023-08-24T05:53:26.749707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.read_csv(\"/kaggle/input/amazon-fine-food-reviews/Reviews.csv\")\nsub=sub.iloc[-50000:]","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:59:26.855065Z","iopub.execute_input":"2023-08-20T10:59:26.855744Z","iopub.status.idle":"2023-08-20T10:59:30.857281Z","shell.execute_reply.started":"2023-08-20T10:59:26.855706Z","shell.execute_reply":"2023-08-20T10:59:30.856169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=sub[\"Score\"].values\nimport seaborn as sns\nsns.displot(sub)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:59:33.447660Z","iopub.execute_input":"2023-08-20T10:59:33.448073Z","iopub.status.idle":"2023-08-20T10:59:34.048144Z","shell.execute_reply.started":"2023-08-20T10:59:33.448037Z","shell.execute_reply":"2023-08-20T10:59:34.047130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ytrain=ytrain-1\nytrain","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:40:06.879118Z","iopub.execute_input":"2023-08-20T18:40:06.879503Z","iopub.status.idle":"2023-08-20T18:40:06.889304Z","shell.execute_reply.started":"2023-08-20T18:40:06.879468Z","shell.execute_reply":"2023-08-20T18:40:06.888237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=sub-1\nsub","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:59:42.033562Z","iopub.execute_input":"2023-08-20T10:59:42.034650Z","iopub.status.idle":"2023-08-20T10:59:42.042873Z","shell.execute_reply.started":"2023-08-20T10:59:42.034597Z","shell.execute_reply":"2023-08-20T10:59:42.041829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ytrain","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:52:29.106268Z","iopub.execute_input":"2023-08-20T18:52:29.106675Z","iopub.status.idle":"2023-08-20T18:52:29.116575Z","shell.execute_reply.started":"2023-08-20T18:52:29.106612Z","shell.execute_reply":"2023-08-20T18:52:29.115189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n#class_weight = 'balanced'\nclasses=[0,1,2]\nweight = compute_class_weight(class_weight='balanced', classes=classes, y=ytrain)\nprint(weight)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:53:43.287110Z","iopub.execute_input":"2023-08-24T05:53:43.287502Z","iopub.status.idle":"2023-08-24T05:53:43.297185Z","shell.execute_reply.started":"2023-08-24T05:53:43.287472Z","shell.execute_reply":"2023-08-24T05:53:43.296092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.utils.class_weight import compute_class_weight\n# # class_weight = 'balanced'\n# classes=[0,1,2,3,4]\n# weight = compute_class_weight(class_weight='balanced', classes=classes, y=sub)\n# print(weight)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:59:50.554043Z","iopub.execute_input":"2023-08-20T10:59:50.554419Z","iopub.status.idle":"2023-08-20T10:59:50.575876Z","shell.execute_reply.started":"2023-08-20T10:59:50.554388Z","shell.execute_reply":"2023-08-20T10:59:50.574296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n# Define your classes and corresponding weights\n# classes = [0, 1, 2, 3, 4]\ncustom_weights = weight  # Define the weights for each class\ncustom_weights=np.array(custom_weights)\nclasses=np.array(classes)\n\n\n# Compute class weights using the custom weights\nytrain_array = np.array(ytrain)\n\nclass_weights = compute_class_weight(class_weight=None, classes=classes, y=ytrain_array)\nclass_weights *= custom_weights  # Multiply with custom weights\n\nprint(\"Custom Class Weights:\", class_weights)\n# class_weight_dict = {cls: weight for cls, weight in zip(classes, custom_weights)}\n\n# # Compute class weights using the custom weights\n# class_weights = compute_class_weight(class_weight=class_weight_dict, classes=classes, y=ytrain)\n\n# print(\"Custom Class Weights:\", class_weights)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:53:48.499883Z","iopub.execute_input":"2023-08-24T05:53:48.500794Z","iopub.status.idle":"2023-08-24T05:53:48.509108Z","shell.execute_reply.started":"2023-08-24T05:53:48.500747Z","shell.execute_reply":"2023-08-24T05:53:48.508025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights=np.array(class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:49:25.430850Z","iopub.execute_input":"2023-08-20T08:49:25.431233Z","iopub.status.idle":"2023-08-20T08:49:25.462934Z","shell.execute_reply.started":"2023-08-20T08:49:25.431200Z","shell.execute_reply":"2023-08-20T08:49:25.461314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import class_weight\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error \nfrom xgboost import XGBClassifier\nxgb_1=XGBClassifier(tree_method='gpu_hist',\n                    predictor='gpu_predictor',n_jobs=-1)\nrmse_scores=[]\nkf = KFold(n_splits = 2)\nfor train_ix, test_ix in tqdm(kf.split(xtrain1)):\n    X_train, X_test = xtrain1[train_ix], xtrain1[test_ix]\n    Y_train, Y_test = ytrain[train_ix], ytrain[test_ix]\n    xgb_1.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=Y_train))\n    pred = xgb_1.predict(X_test)\n    rmse_scores.append(mean_absolute_error(Y_test,pred))\na=np.mean(rmse_scores)\nprint(a)\nxgb_1.fit(xtrain1,ytrain,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=ytrain))","metadata":{"execution":{"iopub.status.busy":"2023-08-20T11:03:28.902814Z","iopub.execute_input":"2023-08-20T11:03:28.903666Z","iopub.status.idle":"2023-08-20T11:03:39.898814Z","shell.execute_reply.started":"2023-08-20T11:03:28.903621Z","shell.execute_reply":"2023-08-20T11:03:39.897814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score  # Import accuracy_score\nfrom sklearn.utils import class_weight\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:53:53.853218Z","iopub.execute_input":"2023-08-24T05:53:53.853576Z","iopub.status.idle":"2023-08-24T05:53:53.858010Z","shell.execute_reply.started":"2023-08-24T05:53:53.853547Z","shell.execute_reply":"2023-08-24T05:53:53.856933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n\nprint(\"Custom Class Weights:\", class_weight_dict)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:53:56.193635Z","iopub.execute_input":"2023-08-24T05:53:56.194038Z","iopub.status.idle":"2023-08-24T05:53:56.205025Z","shell.execute_reply.started":"2023-08-24T05:53:56.194005Z","shell.execute_reply":"2023-08-24T05:53:56.203984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain1.size","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:54:01.739431Z","iopub.execute_input":"2023-08-24T05:54:01.739785Z","iopub.status.idle":"2023-08-24T05:54:01.746023Z","shell.execute_reply.started":"2023-08-24T05:54:01.739754Z","shell.execute_reply":"2023-08-24T05:54:01.745013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtest1.size","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:54:11.885667Z","iopub.execute_input":"2023-08-24T05:54:11.886050Z","iopub.status.idle":"2023-08-24T05:54:11.892280Z","shell.execute_reply.started":"2023-08-24T05:54:11.886019Z","shell.execute_reply":"2023-08-24T05:54:11.891251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text1","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:35:30.403211Z","iopub.execute_input":"2023-08-24T05:35:30.403624Z","iopub.status.idle":"2023-08-24T05:35:30.412361Z","shell.execute_reply.started":"2023-08-24T05:35:30.403592Z","shell.execute_reply":"2023-08-24T05:35:30.411228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom transformers import AdamW\nfrom tqdm import tqdm\nclass BertClassificationModel(nn.Module):\n    def __init__(self,hidden_size=768,num_class=3): \n        super(BertClassificationModel, self).__init__()\n        model_name = \"siebert/sentiment-roberta-large-english\"\n        self.tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n        self.bert = RobertaModel.from_pretrained(pretrained_model_name_or_path=model_name)\n\n        for p in self.bert.parameters(): \n                p.requires_grad = False\n        self.fc = nn.Linear(hidden_size, num_class)\n\n    def forward(self, batch_sentences): \n        sentences_tokenizer = self.tokenizer(batch_sentences,\n                                             truncation=True,\n                                             padding=True,\n                                             max_length=512,\n                                             add_special_tokens=True)\n        input_ids=torch.tensor(sentences_tokenizer['input_ids']).to(device) \n        attention_mask=torch.tensor(sentences_tokenizer['attention_mask']).to(device) \n        bert_out=self.bert(input_ids=input_ids,attention_mask=attention_mask) \n\n        last_hidden_state =bert_out[0].to(device) \n        bert_cls_hidden_state=last_hidden_state[:,0,:].to(device) \n        return bert_cls_hidden_state\nmodel=BertClassificationModel()\nmodel=model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:40:44.118029Z","iopub.execute_input":"2023-08-24T05:40:44.118941Z","iopub.status.idle":"2023-08-24T05:40:48.689393Z","shell.execute_reply.started":"2023-08-24T05:40:44.118895Z","shell.execute_reply":"2023-08-24T05:40:48.688171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaForSequenceClassification\n#xtrain, ytrain,xtest, ytest = train_test_split(train_text1, test_text1, test_size=0.2, random_state=42)\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\nytrain_tensor = torch.tensor(ytrain)\nytest_tensor = torch.tensor(ytest)\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\ntrain_encodings = tokenizer(xtrain, truncation=True, padding=True, return_tensors=\"pt\")\ntest_encodings = tokenizer(xtest, truncation=True, padding=True, return_tensors=\"pt\")\n# Training loop\nfor epoch in range(5):  # Example: 5 epochs\n    model.train()\n    optimizer.zero_grad()\n    train_outputs = model(**train_encodings, labels=ytrain_tensor)\n    loss = train_outputs.loss\n    loss.backward()\n    optimizer.step()\n\n# Evaluation\nmodel.eval()\nwith torch.no_grad():\n    test_outputs = model(**test_encodings)\n    predicted_classes = torch.argmax(test_outputs.logits, dim=1)\n    accuracy = accuracy_score(ytest, predicted_classes)\n\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:41:21.965732Z","iopub.execute_input":"2023-08-24T05:41:21.966138Z","iopub.status.idle":"2023-08-24T05:41:23.590410Z","shell.execute_reply.started":"2023-08-24T05:41:21.966104Z","shell.execute_reply":"2023-08-24T05:41:23.588998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb_1=XGBClassifier(tree_method='gpu_hist',\n                    predictor='gpu_predictor',n_jobs=-1)\naccuracy_scores=[]\nkf = KFold(n_splits = 3)\nfor train_ix, test_ix in tqdm(kf.split(xtrain1)):\n    X_train, X_test = xtrain1[train_ix], xtrain1[test_ix]\n    Y_train, Y_test = ytrain[train_ix], ytrain[test_ix]\n    xgb_1.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight=class_weight_dict, y=Y_train))\n    pred = xgb_1.predict(X_test)\n    accuracy = accuracy_score(Y_test, pred)  # Calculate accuracy for each fold\n    print(accuracy)\n    accuracy_scores.append(accuracy)\n\nmean_accuracy = np.mean(accuracy_scores)\nprint(\"Mean Accuracy:\", mean_accuracy)\n\n# Train the final model on the entire dataset\nxgb_1.fit(xtrain1, ytrain, sample_weight=class_weight.compute_sample_weight(class_weight=class_weight_dict, y=ytrain))","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:17:34.862523Z","iopub.execute_input":"2023-08-24T05:17:34.862913Z","iopub.status.idle":"2023-08-24T05:17:35.373501Z","shell.execute_reply.started":"2023-08-24T05:17:34.862881Z","shell.execute_reply":"2023-08-24T05:17:35.370182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb_1=XGBClassifier(tree_method='gpu_hist',\n                    predictor='gpu_predictor',n_jobs=-1)\nxgb_1.fit(xtrain1, ytrain, sample_weight=class_weight.compute_sample_weight(class_weight=class_weight_dict, y=ytrain))","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:15:05.039573Z","iopub.execute_input":"2023-08-24T05:15:05.040013Z","iopub.status.idle":"2023-08-24T05:15:07.689090Z","shell.execute_reply.started":"2023-08-24T05:15:05.039956Z","shell.execute_reply":"2023-08-24T05:15:07.687248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_1.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight=class_weight_dict, y=Y_train))\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:28:22.520695Z","iopub.execute_input":"2023-08-23T20:28:22.521084Z","iopub.status.idle":"2023-08-23T20:28:22.572896Z","shell.execute_reply.started":"2023-08-23T20:28:22.521053Z","shell.execute_reply":"2023-08-23T20:28:22.571633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = xgb_1.predict(X_test)\naccuracy = accuracy_score(Y_test, pred)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T07:03:49.755378Z","iopub.execute_input":"2023-08-21T07:03:49.755768Z","iopub.status.idle":"2023-08-21T07:03:53.511315Z","shell.execute_reply.started":"2023-08-21T07:03:49.755736Z","shell.execute_reply":"2023-08-21T07:03:53.510218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T07:04:00.009975Z","iopub.execute_input":"2023-08-21T07:04:00.011225Z","iopub.status.idle":"2023-08-21T07:04:00.018734Z","shell.execute_reply.started":"2023-08-21T07:04:00.011179Z","shell.execute_reply":"2023-08-21T07:04:00.017678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test = xtrain1[train_ix], xtrain1[test_ix]\nY_train, Y_test = ytrain[train_ix], ytrain[test_ix]\nxgb_1.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight=class_weight_dict, y=Y_train))\npred = xgb_1.predict(X_test)\naccuracy = accuracy_score(Y_test, pred)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T19:58:01.934321Z","iopub.execute_input":"2023-08-20T19:58:01.934877Z","iopub.status.idle":"2023-08-20T19:58:15.094561Z","shell.execute_reply.started":"2023-08-20T19:58:01.934823Z","shell.execute_reply":"2023-08-20T19:58:15.093214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_2=XGBClassifier(tree_method='gpu_hist',\n                    predictor='gpu_predictor',n_jobs=-1)\nrmse_scores=[]\nfor train_ix, test_ix in tqdm(kf.split(xtrain2)):\n    X_train, X_test = xtrain2[train_ix], xtrain2[test_ix]\n    Y_train, Y_test = ytrain[train_ix], ytrain[test_ix]\n    xgb_2.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=Y_train))\n    pred = xgb_2.predict(X_test)\n    rmse_scores.append(mean_absolute_error(Y_test,pred))\nb=np.mean(rmse_scores)\nprint(b)\nxgb_2.fit(xtrain2,ytrain,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=ytrain))","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:50:59.472056Z","iopub.execute_input":"2023-08-20T18:50:59.472540Z","iopub.status.idle":"2023-08-20T18:50:59.523249Z","shell.execute_reply.started":"2023-08-20T18:50:59.472496Z","shell.execute_reply":"2023-08-20T18:50:59.520698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogic_1=LogisticRegression()\nrmse_scores=[]\nfor train_ix, test_ix in tqdm(kf.split(xtrain1)):\n    X_train, X_test = xtrain1[train_ix], xtrain1[test_ix]\n    Y_train, Y_test = ytrain[train_ix], ytrain[test_ix]\n    logic_1.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=Y_train))\n    pred = logic_1.predict(X_test)\n    rmse_scores.append(mean_absolute_error(Y_test,pred))\nc=np.mean(rmse_scores)\nprint(c)\nlogic_1.fit(xtrain1,ytrain,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=ytrain))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:17:51.126801Z","iopub.execute_input":"2023-02-08T11:17:51.127143Z","iopub.status.idle":"2023-02-08T11:19:07.326665Z","shell.execute_reply.started":"2023-02-08T11:17:51.12711Z","shell.execute_reply":"2023-02-08T11:19:07.325271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logic_2=LogisticRegression()\nrmse_scores=[]\nfor train_ix, test_ix in tqdm(kf.split(xtrain2)):\n    X_train, X_test = xtrain2[train_ix], xtrain2[test_ix]\n    Y_train, Y_test = ytrain[train_ix], ytrain[test_ix]\n    logic_2.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=Y_train))\n    pred = logic_2.predict(X_test)\n    rmse_scores.append(mean_absolute_error(Y_test,pred))\nd=np.mean(rmse_scores)\nprint(d)\nlogic_2.fit(xtrain2,ytrain,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=ytrain))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:19:07.328268Z","iopub.execute_input":"2023-02-08T11:19:07.328672Z","iopub.status.idle":"2023-02-08T11:19:51.170653Z","shell.execute_reply.started":"2023-02-08T11:19:07.328621Z","shell.execute_reply":"2023-02-08T11:19:51.169412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred1=xgb_1.predict_proba(xtest1)\ny1=xgb_1.predict(xtest1)\nsns.displot(y1)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:19:51.176109Z","iopub.execute_input":"2023-02-08T11:19:51.176803Z","iopub.status.idle":"2023-02-08T11:19:53.034551Z","shell.execute_reply.started":"2023-02-08T11:19:51.176751Z","shell.execute_reply":"2023-02-08T11:19:53.03353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred2=xgb_2.predict_proba(xtest2)\ny2=xgb_2.predict(xtest2)\nsns.displot(y2)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:19:53.035996Z","iopub.execute_input":"2023-02-08T11:19:53.036365Z","iopub.status.idle":"2023-02-08T11:19:53.719573Z","shell.execute_reply.started":"2023-02-08T11:19:53.036328Z","shell.execute_reply":"2023-02-08T11:19:53.718571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred3=logic_1.predict_proba(xtest1)\ny3=logic_1.predict(xtest1)\nsns.displot(y3)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:19:53.72367Z","iopub.execute_input":"2023-02-08T11:19:53.725979Z","iopub.status.idle":"2023-02-08T11:19:54.561495Z","shell.execute_reply.started":"2023-02-08T11:19:53.725936Z","shell.execute_reply":"2023-02-08T11:19:54.560524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred4=logic_2.predict_proba(xtest2)\ny4=logic_2.predict(xtest2)\nsns.displot(y4)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:19:54.563025Z","iopub.execute_input":"2023-02-08T11:19:54.56339Z","iopub.status.idle":"2023-02-08T11:19:54.898507Z","shell.execute_reply.started":"2023-02-08T11:19:54.563354Z","shell.execute_reply":"2023-02-08T11:19:54.897558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nlgb_1=LGBMClassifier(device=\"gpu\",\n                   gpu_platform_id=0,\n                   gpu_device_id=0)\nrmse_scores=[]\nfor train_ix, test_ix in tqdm(kf.split(xtrain1)):\n    X_train, X_test = xtrain1[train_ix], xtrain1[test_ix]\n    Y_train, Y_test = ytrain[train_ix], ytrain[test_ix]\n    lgb_1.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=Y_train))\n    pred = lgb_1.predict(X_test)\n    rmse_scores.append(mean_absolute_error(Y_test,pred))\ne=np.mean(rmse_scores)\nprint(e)\nlgb_1.fit(xtrain1,ytrain,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=ytrain))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:19:54.900004Z","iopub.execute_input":"2023-02-08T11:19:54.900344Z","iopub.status.idle":"2023-02-08T11:24:40.638358Z","shell.execute_reply.started":"2023-02-08T11:19:54.90031Z","shell.execute_reply":"2023-02-08T11:24:40.637388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_2=LGBMClassifier(device=\"gpu\",\n                   gpu_platform_id=0,\n                   gpu_device_id=0)\nrmse_scores=[]\nfor train_ix, test_ix in tqdm(kf.split(xtrain2)):\n    X_train, X_test = xtrain2[train_ix], xtrain2[test_ix]\n    Y_train, Y_test = ytrain[train_ix], ytrain[test_ix]\n    lgb_2.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=Y_train))\n    pred = lgb_2.predict(X_test)\n    rmse_scores.append(mean_absolute_error(Y_test,pred))\nf=np.mean(rmse_scores)\nprint(f)\nlgb_2.fit(xtrain2,ytrain,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=ytrain))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:24:40.641329Z","iopub.execute_input":"2023-02-08T11:24:40.641603Z","iopub.status.idle":"2023-02-08T11:28:28.822471Z","shell.execute_reply.started":"2023-02-08T11:24:40.641577Z","shell.execute_reply":"2023-02-08T11:28:28.821527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\ncat_1=CatBoostClassifier(logging_level='Silent',\n                         task_type='GPU',loss_function='MultiClass')\nrmse_scores=[]\nfor train_ix, test_ix in tqdm(kf.split(xtrain1)):\n    X_train, X_test = xtrain1[train_ix], xtrain1[test_ix]\n    Y_train, Y_test = ytrain[train_ix], ytrain[test_ix]\n    cat_1.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=Y_train))\n    pred = cat_1.predict(X_test)\n    rmse_scores.append(mean_absolute_error(Y_test,pred))\ng=np.mean(rmse_scores)\nprint(g)\ncat_1.fit(xtrain1,ytrain,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=ytrain))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:38:06.380448Z","iopub.execute_input":"2023-02-08T11:38:06.380929Z","iopub.status.idle":"2023-02-08T11:41:18.769362Z","shell.execute_reply.started":"2023-02-08T11:38:06.380889Z","shell.execute_reply":"2023-02-08T11:41:18.768567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_2=CatBoostClassifier(logging_level='Silent',\n                         task_type='GPU',loss_function='MultiClass')\nrmse_scores=[]\nfor train_ix, test_ix in tqdm(kf.split(xtrain2)):\n    X_train, X_test = xtrain2[train_ix], xtrain2[test_ix]\n    Y_train, Y_test = ytrain[train_ix], ytrain[test_ix]\n    cat_2.fit(X_train,Y_train,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=Y_train))\n    pred = cat_2.predict(X_test)\n    rmse_scores.append(mean_absolute_error(Y_test,pred))\nh=np.mean(rmse_scores)\nprint(h)\ncat_2.fit(xtrain2,ytrain,sample_weight=class_weight.compute_sample_weight(class_weight='balanced', y=ytrain))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:41:18.773242Z","iopub.execute_input":"2023-02-08T11:41:18.7741Z","iopub.status.idle":"2023-02-08T11:45:02.718327Z","shell.execute_reply.started":"2023-02-08T11:41:18.774061Z","shell.execute_reply":"2023-02-08T11:45:02.717521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred5=lgb_1.predict_proba(xtest1)\ny5=lgb_1.predict(xtest1)\nsns.displot(y5)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:45:02.722094Z","iopub.execute_input":"2023-02-08T11:45:02.724002Z","iopub.status.idle":"2023-02-08T11:45:06.850652Z","shell.execute_reply.started":"2023-02-08T11:45:02.723966Z","shell.execute_reply":"2023-02-08T11:45:06.84963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred6=lgb_2.predict_proba(xtest2)\ny6=lgb_2.predict(xtest2)\nsns.displot(y6)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:45:06.853366Z","iopub.execute_input":"2023-02-08T11:45:06.853772Z","iopub.status.idle":"2023-02-08T11:45:10.838536Z","shell.execute_reply.started":"2023-02-08T11:45:06.853732Z","shell.execute_reply":"2023-02-08T11:45:10.83754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred7=cat_1.predict_proba(xtest1)\ny7=cat_1.predict(xtest1)\nsns.displot(y7)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:45:10.840043Z","iopub.execute_input":"2023-02-08T11:45:10.840392Z","iopub.status.idle":"2023-02-08T11:45:14.123526Z","shell.execute_reply.started":"2023-02-08T11:45:10.840357Z","shell.execute_reply":"2023-02-08T11:45:14.12255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred8=cat_2.predict_proba(xtest2)\ny8=cat_2.predict(xtest2)\nsns.displot(y8)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:45:14.125147Z","iopub.execute_input":"2023-02-08T11:45:14.12551Z","iopub.status.idle":"2023-02-08T11:45:16.371569Z","shell.execute_reply.started":"2023-02-08T11:45:14.125475Z","shell.execute_reply":"2023-02-08T11:45:16.37054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch_tabnet","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:45:16.372915Z","iopub.execute_input":"2023-02-08T11:45:16.373734Z","iopub.status.idle":"2023-02-08T11:45:28.056716Z","shell.execute_reply.started":"2023-02-08T11:45:16.373695Z","shell.execute_reply":"2023-02-08T11:45:28.055556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w=ytrain.copy()\nw=w.astype(\"float32\")\nfor i in range(ytrain.shape[0]):\n    w[i]=weight[ytrain[i]]\nw","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:45:28.058759Z","iopub.execute_input":"2023-02-08T11:45:28.059147Z","iopub.status.idle":"2023-02-08T11:45:28.099755Z","shell.execute_reply.started":"2023-02-08T11:45:28.059105Z","shell.execute_reply":"2023-02-08T11:45:28.098701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetClassifier\ntab_1=TabNetClassifier()\nrmse_scores=[]\nfor train_ix, test_ix in tqdm(kf.split(xtrain1)):\n    X_train, X_test = xtrain1[train_ix], xtrain1[test_ix]\n    Y_train, Y_test = ytrain[train_ix], ytrain[test_ix]\n    ww=w[train_ix]\n    tab_1.fit(X_train,Y_train,weights=ww,eval_metric=['mae'])\n    pred = tab_1.predict(X_test)\n    rmse_scores.append(mean_absolute_error(Y_test,pred))\ni=np.mean(rmse_scores)\nprint(i)\ntab_1.fit(xtrain1,ytrain,weights=w,eval_metric=['mae'])","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:45:28.10117Z","iopub.execute_input":"2023-02-08T11:45:28.101608Z","iopub.status.idle":"2023-02-08T11:57:28.001604Z","shell.execute_reply.started":"2023-02-08T11:45:28.101572Z","shell.execute_reply":"2023-02-08T11:57:28.000671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred9=tab_1.predict_proba(xtest1)\ny9=tab_1.predict(xtest1)\nsns.displot(y9)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:57:28.007878Z","iopub.execute_input":"2023-02-08T11:57:28.01007Z","iopub.status.idle":"2023-02-08T11:57:30.159589Z","shell.execute_reply.started":"2023-02-08T11:57:28.010033Z","shell.execute_reply":"2023-02-08T11:57:30.158553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=np.argmax((ypred1/a+ypred2/b+ypred3/c+ypred4/d+ypred5/e+ypred6/f+ypred7/g+ypred8/h+ypred9/i)/(1/a+1/b+1/c+1/d+1/e+1/f+1/g+1/h+1/i),axis=1)+1\nsns.displot(y)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:57:30.160916Z","iopub.execute_input":"2023-02-08T11:57:30.161585Z","iopub.status.idle":"2023-02-08T11:57:30.462935Z","shell.execute_reply.started":"2023-02-08T11:57:30.161546Z","shell.execute_reply":"2023-02-08T11:57:30.462061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**And also you can try regressor to do,because we use MAE to judge our model actually**","metadata":{}},{"cell_type":"code","source":"sub[\"Rating\"]=y\nsub","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:57:30.464182Z","iopub.execute_input":"2023-02-08T11:57:30.465106Z","iopub.status.idle":"2023-02-08T11:57:30.477016Z","shell.execute_reply.started":"2023-02-08T11:57:30.465069Z","shell.execute_reply":"2023-02-08T11:57:30.475968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"sample_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T11:57:30.478439Z","iopub.execute_input":"2023-02-08T11:57:30.478942Z","iopub.status.idle":"2023-02-08T11:57:30.522414Z","shell.execute_reply.started":"2023-02-08T11:57:30.478905Z","shell.execute_reply":"2023-02-08T11:57:30.521549Z"},"trusted":true},"execution_count":null,"outputs":[]}]}